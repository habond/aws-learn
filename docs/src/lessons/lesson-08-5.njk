---
layout: lesson-layout.njk
permalink: lessons/lesson-08-5.html
lessonNumber: 8
lessonTitle: "Big Data Analytics"
duration: "~7 hours"
cost: "$3-5"
stepNumber: 5
totalSteps: 5
progress: 100
title: "Lesson 8: Real-time Processing with Lambda - AWS Learning Tutorial"
tocContent: |
  <li>
  <a href="lesson-08.html">
  <span class="step-number">1</span>
  <span class="step-title">Overview + Kinesis Data Streams</span>
  </a>
  </li>
  <li>
  <a href="lesson-08-2.html">
  <span class="step-number">2</span>
  <span class="step-title">Kinesis Firehose to S3</span>
  </a>
  </li>
  <li>
  <a href="lesson-08-3.html">
  <span class="step-number">3</span>
  <span class="step-title">AWS Glue (Data Catalog)</span>
  </a>
  </li>
  <li>
  <a href="lesson-08-4.html">
  <span class="step-number">4</span>
  <span class="step-title">Athena Queries</span>
  </a>
  </li>
  <li class="active">
  <a href="lesson-08-5.html">
  <span class="step-number">5</span>
  <span class="step-title">Real-time Processing with Lambda</span>
  </a>
  </li>
---

<div class="step-header">
        <span class="step-badge">Lesson 8 - Part 5</span>
        <h1>Real-time Processing with Lambda</h1>
      </div>

      <div class="step-content">
        <h2>Part 5: Real-time Processing with Lambda (1 hour)</h2>

        <h3>Step 12: Create Lambda for Kinesis Processing</h3>
        <p>Create <code>processor/index.js</code>:</p>

        <pre><code class="language-javascript">const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
const s3 = new S3Client({});

exports.handler = async (event) =&gt; {
  console.log(`Processing ${event.Records.length} records`);

  const alerts = [];

  for (const record of event.Records) {
    const payload = Buffer.from(record.kinesis.data, 'base64').toString('utf-8');
    const data = JSON.parse(payload);

    console.log('Processing:', data);

    // Alert on high temperature
    if (data.temperature &gt; 28.0) {
      alerts.push({
        sensor_id: data.sensor_id,
        location: data.location,
        temperature: data.temperature,
        timestamp: data.timestamp,
        alert_type: 'HIGH_TEMPERATURE'
      });
    }

    // Alert on high humidity
    if (data.humidity &gt; 75.0) {
      alerts.push({
        sensor_id: data.sensor_id,
        location: data.location,
        humidity: data.humidity,
        timestamp: data.timestamp,
        alert_type: 'HIGH_HUMIDITY'
      });
    }
  }

  // Save alerts to S3
  if (alerts.length &gt; 0) {
    const alertFile = `alerts/${new Date().toISOString()}.json`;

    await s3.send(new PutObjectCommand({
      Bucket: process.env.BUCKET_NAME,
      Key: alertFile,
      Body: JSON.stringify(alerts, null, 2),
      ContentType: 'application/json'
    }));

    console.log(`Saved ${alerts.length} alerts to ${alertFile}`);
  }

  return {
    statusCode: 200,
    body: JSON.stringify({
      processed: event.Records.length,
      alerts: alerts.length
    })
  };
};</code></pre>

        <p>Deploy:</p>

        <pre><code class="language-bash">cd processor
npm init -y
npm install @aws-sdk/client-s3
zip -r function.zip .

# Create role (or reuse existing)
# Deploy function
aws lambda create-function \
  --function-name sensor-stream-processor \
  --runtime nodejs20.x \
  --role $LAMBDA_ROLE_ARN \
  --handler index.handler \
  --zip-file fileb://function.zip \
  --timeout 60 \
  --environment "Variables={BUCKET_NAME=$BUCKET_NAME}"

# Create event source mapping
aws lambda create-event-source-mapping \
  --function-name sensor-stream-processor \
  --event-source-arn $STREAM_ARN \
  --starting-position LATEST \
  --batch-size 100</code></pre>

        <h3>Step 13: Monitor Processing</h3>
        <pre><code class="language-bash"># Check Lambda logs
aws logs tail /aws/lambda/sensor-stream-processor --follow

# Check for alerts in S3
aws s3 ls s3://$BUCKET_NAME/alerts/</code></pre>

        <hr style="margin: 40px 0; border: none; border-top: 2px solid var(--border-color);">

        <h2>Challenges (Optional)</h2>

        <h3>Easy</h3>
        <ul>
          <li>Create CloudWatch dashboard for stream metrics</li>
          <li>Query data by date partition</li>
          <li>Add more sensor types</li>
        </ul>

        <h3>Medium</h3>
        <ul>
          <li>Set up CloudWatch alarms for anomalies</li>
          <li>Create Glue ETL job to clean data</li>
          <li>Build QuickSight dashboard for visualization</li>
          <li>Implement data retention policy</li>
        </ul>

        <h3>Hard</h3>
        <ul>
          <li>Add machine learning anomaly detection (SageMaker)</li>
          <li>Implement late-arriving data handling</li>
          <li>Create real-time aggregations with Kinesis Analytics</li>
          <li>Build Lambda data enrichment pipeline</li>
        </ul>

        <hr style="margin: 40px 0; border: none; border-top: 2px solid var(--border-color);">

        <h2>Troubleshooting</h2>

        <h3>Kinesis stream not receiving data?</h3>
        <ul>
          <li>Check producer is running</li>
          <li>Verify AWS credentials</li>
          <li>Check stream exists</li>
        </ul>

        <h3>Firehose not delivering to S3?</h3>
        <ul>
          <li>Check IAM role permissions</li>
          <li>Verify S3 bucket exists</li>
          <li>Wait for buffer time (60 seconds)</li>
        </ul>

        <h3>Glue crawler not finding data?</h3>
        <ul>
          <li>Verify S3 path is correct</li>
          <li>Check data format (JSON)</li>
          <li>Ensure IAM role has S3 access</li>
        </ul>

        <h3>Athena queries failing?</h3>
        <ul>
          <li>Check table schema in Glue catalog</li>
          <li>Verify S3 result location</li>
          <li>Check query syntax</li>
        </ul>

        <hr style="margin: 40px 0; border: none; border-top: 2px solid var(--border-color);">

        <h2>Cleanup</h2>

        <pre><code class="language-bash"># Stop producer
kill $PRODUCER_PID

# Delete event source mapping
aws lambda list-event-source-mappings \
  --function-name sensor-stream-processor \
  --query 'EventSourceMappings[0].UUID' \
  --output text | xargs -I {} aws lambda delete-event-source-mapping --uuid {}

# Delete Lambda
aws lambda delete-function --function-name sensor-stream-processor

# Delete Firehose
aws firehose delete-delivery-stream --delivery-stream-name sensor-data-delivery

# Delete Kinesis stream
aws kinesis delete-stream --stream-name sensor-data-stream

# Delete Glue resources
aws glue delete-crawler --name sensor-data-crawler
aws glue delete-database --name sensor_analytics

# Delete S3 buckets
aws s3 rm s3://$BUCKET_NAME --recursive
aws s3 rb s3://$BUCKET_NAME
aws s3 rm s3://$RESULTS_BUCKET --recursive
aws s3 rb s3://$RESULTS_BUCKET

# Delete Athena workgroup
aws athena delete-work-group --work-group sensor-analytics-workgroup

# Delete IAM roles and policies
# (cleanup commands...)</code></pre>

        <hr style="margin: 40px 0; border: none; border-top: 2px solid var(--border-color);">

        <h2>What You Learned</h2>

        <ul>
          <li>Ingested streaming data with Kinesis</li>
          <li>Delivered data to S3 with Firehose</li>
          <li>Cataloged data with Glue</li>
          <li>Queried data with Athena (serverless SQL)</li>
          <li>Processed streams in real-time with Lambda</li>
          <li>Built complete analytics pipeline</li>
          <li>Implemented data lake architecture</li>
        </ul>

        <hr style="margin: 40px 0; border: none; border-top: 2px solid var(--border-color);">

        <h2>Next Steps</h2>

        <p>Head to <a href="lesson-09.html">Lesson 9: AI/ML Buzzwords</a> to add machine learning to your applications with SageMaker, Rekognition, or Bedrock!</p>
      </div>

      <!-- Navigation -->
      <div class="step-navigation">
        <div class="nav-button-group">
          <a href="lesson-08-4.html" class="btn btn-secondary">← Previous: Athena Queries</a>
          <a href="lesson-09.html" class="btn btn-primary">Next Lesson: AI/ML Buzzwords →</a>
        </div>
      </div>